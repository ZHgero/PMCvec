{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing and chunking\n",
    "\n",
    "import re\n",
    "\n",
    "def load_stop_words():\n",
    "    return set(line.strip() for line in open('stopwords-en.txt'))\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    \"\"\"remove characters that are not indicators of phrase boundaries\"\"\"\n",
    "    return re.sub(\"([{}@\\\"$%&\\\\\\/*'\\\"]|\\d)\", \"\", text)\n",
    "    \n",
    "\n",
    "def generate_candidate_phrases(text):\n",
    "    \"\"\" generate phrases using phrase boundary markers \"\"\"\n",
    "    coarse_candidates = char_splitter.split(text.lower().replace('(',\" to \").replace(')',\" to \"))\n",
    "    \n",
    "    candidate_phrases = []\n",
    "\n",
    "    for coarse_phrase in coarse_candidates:\n",
    "\n",
    "        words = re.split(\"\\\\s+\", coarse_phrase)\n",
    "        previous_stop = False\n",
    "\n",
    "        for w in words:\n",
    "            if (w in stopwords ) and not previous_stop:\n",
    "                candidate_phrases.append(\";\")\n",
    "                previous_stop = True\n",
    "            elif w not in stopwords and len(w) >=3:\n",
    "                candidate_phrases.append(w.strip())\n",
    "                previous_stop = False\n",
    "\n",
    "    phrases = re.split(\";+\", ' '.join(candidate_phrases))\n",
    "    return phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering out special characters and numbers\n",
    "with open('preprocessed.txt','a') as outfile:\n",
    "    outfile.write(remove_special_characters(abstracts)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating candidate phrases\n",
    "char_splitter = re.compile(\"[.,;!:-]\")\n",
    "stopwords = load_stop_words()\n",
    "with open('candidates.txt','w') as outfile:\n",
    "    with open('preprocessed.txt') as infile:\n",
    "        for line in infile:\n",
    "            phr=generate_candidate_phrases(line)\n",
    "            for i in range(len(phr)):\n",
    "                if len(phr[i].split())>1:\n",
    "                    outfile.write(phr[i])\n",
    "                    outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of occurrences for each phrase\n",
    "clean_phrases={}\n",
    "with open('candidates.txt','r') as infile:\n",
    "    with open('unique.txt','w') as outfile:\n",
    "        for line in infile:\n",
    "            sline= line.strip()\n",
    "            if sline not in clean_phrases:\n",
    "                clean_phrases[sline]=0\n",
    "                outfile.write(sline)\n",
    "                outfile.write('\\n')\n",
    "                clean_phrases[sline]+=1\n",
    "\n",
    "#filter out phrases that dont occur less than 100 times\n",
    "with open('unique.txt','r') as myfile:\n",
    "    with open('top_phrases.txt','w') as outfile:\n",
    "        for line in myfile :\n",
    "            sline= line.strip()\n",
    "            if clean_phrases[sline] >=100:\n",
    "                outfile.write(sline)\n",
    "                outfile.write(\"<>\")\n",
    "                outfile.write(str(clean_phrases[sline]))\n",
    "                outfile.write('\\n')\n",
    "                \n",
    "#frequency for the filtered phrases\n",
    "phrase_frequency= {}\n",
    "with open('unique.txt','r') as infile:\n",
    "    for line in infile:\n",
    "        sline=line.split('<>')\n",
    "        if sline[0] not in phrase_frequency:\n",
    "            phrase_frequency[sline[0]]=sline[1]\n",
    "        else: \n",
    "            phrase_frequency[sline[0]]=int(phrase_frequency[sline[0]])+int(sline[1])    \n",
    "\n",
    "#write the phrases and their frequency to file            \n",
    "with open('all_phrases.txt','w') as outfile:\n",
    "    for k, v in phrase_frequency.iteritems():\n",
    "        outfile.write(k+'<>'+str(v))\n",
    "        outfile.write('\\n')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count word frequencies for words in our phrases\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "counter=Counter()\n",
    "f1= open('all_phrases.txt') \n",
    "lin= set(f1.read().strip().split())   \n",
    "with open('preprocessed.txt') as f2:\n",
    "    lines= (line.lower().split() for line in f2)\n",
    "    for item in lines:\n",
    "        counter.update(Counter([s for s in item if s in lin]))\n",
    "            \n",
    "\n",
    "#calculate phrase importance using information frequency\n",
    "N=1886096272 # total number of words\n",
    "info_freq={}\n",
    "\n",
    "for key, value in sorted(phrases.iteritems(), key=lambda (k,v): (v,k)):\n",
    "    \n",
    "    mnm= key.split()\n",
    "    \n",
    "    count=0\n",
    "    multiple=1.0\n",
    "    minus=0\n",
    "    n=len(mnm)\n",
    "    for i in range(n):\n",
    "        try:\n",
    "            count+=counter[mnm[i]]\n",
    "            multiple*=counter[mnm[i]]/float(N)\n",
    "        except KeyError: \n",
    "            pass\n",
    "    \n",
    "    try:\n",
    "        val=float(\"{0:.2f}\".format(np.log(value/multiple*N)))\n",
    "        info_freq[key]=val*np.log(value)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag selected phrases back to the preprocessed file\n",
    "\n",
    "phrases=all_phrases[:18000]\n",
    "def tagg_data(original_text, phrases):\n",
    "    for phrase in phrases:\n",
    "        sphrase= phrase.split()\n",
    "        replacement= '_'.join(sphrase)\n",
    "        \n",
    "        original_text = original_text.replace(phrase,replacement)\n",
    "    return original_text   \n",
    "\n",
    "def which_phrases(item):       \n",
    "    return [f for f in phrases if f in item]\n",
    "\n",
    "\n",
    "from itertools import islice\n",
    "start= time.time()\n",
    "\n",
    "with open('preprocessed.txt') as infile:\n",
    "    with open('tagged.txt','a') as outfile:\n",
    "        for item in infile:\n",
    "            item= item.lower()\n",
    "            select_phrases=which_phrases(item)\n",
    "            t_item=tagg_data(item, select_phrases)\n",
    "\n",
    "            for line in t_item:\n",
    "                outfile.write(line)  \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a word embedding model using the tagged data\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "import codecs, time, re\n",
    "import glob, gzip, os\n",
    "with codecs.open('tagged.txt','r', errors='ignore') as text_doc: \n",
    "    texts = [[word for word in document.lower().split() if word not in stop_words and re.match(\"^[a-zA-Z_]*$\", word)] for document in text_doc.read().split('.')]\n",
    "model = gensim.models.Word2Vec(texts, size=200, window=15, min_count=1, workers=8, negative=10, sample=1e-5)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
